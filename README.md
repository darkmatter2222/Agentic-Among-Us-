# Agentic Among Us

An AI-powered simulation of **Among Us** where autonomous LLM-driven agents play as Crewmates and Impostors. Watch AI agents navigate The Skeld, complete tasks, form alliances, spread rumors, and (eventually) eliminate each otherâ€”all powered by large language models.

## ğŸ® What Is This?

This project creates a fully autonomous Among Us simulation where:
- **8 AI agents** (6 Crewmates, 2 Impostors) make real-time decisions using LLM reasoning
- Agents have **memory systems** tracking observations, suspicions, and conversations
- Natural **speech and social interactions** between agents
- Full **pathfinding and collision avoidance** on The Skeld map
- Real-time **visualization** via React + PixiJS client

## âœ… Currently Implemented

| Feature | Status | Details |
|---------|--------|---------|
| **Movement & Pathfinding** | âœ… Complete | A* on visibility graph, steering behaviors, collision avoidance |
| **Navigation Mesh** | âœ… Complete | Full Skeld map with walkable zones, rooms, and hallways |
| **Task System** | âœ… Complete | Task assignment, navigation, execution with realistic durations |
| **Vision System** | âœ… Complete | Agents see only within configurable vision radius |
| **AI Decision Making** | âœ… Complete | LLM-powered goals: tasks, wandering, following, avoiding, confronting |
| **Agent Memory** | âœ… Complete | Observations, suspicion tracking, conversation history |
| **Speech System** | âœ… Complete | Agents speak to nearby players, hear within radius |
| **Social Actions** | âœ… Complete | Buddy up, follow, avoid, confront, spread rumors, defend self |
| **Thought System** | âœ… Complete | Internal reasoning triggered by events (entering rooms, spotting agents) |
| **Impostor Task Faking** | âœ… Complete | Impostors fake tasks without contributing to task bar |
| **WebSocket Streaming** | âœ… Complete | Real-time state sync with delta compression |
| **PixiJS Visualization** | âœ… Complete | Map, agents, vision cones, paths, speech bubbles, info panels |

## ğŸš§ Planned / Not Yet Implemented

| Feature | Status |
|---------|--------|
| Kill System | âŒ Not implemented |
| Body Discovery & Reporting | âŒ Not implemented |
| Emergency Meetings | âŒ Not implemented |
| Discussion & Voting | âŒ Not implemented |
| Ejection Mechanics | âŒ Not implemented |
| Sabotage System | âŒ Not implemented |
| Vent System | âŒ Not implemented |
| Door System | âŒ Not implemented |
| Win Conditions | âŒ Not implemented |
| Ghost Mode | âŒ Not implemented |

---

## Prerequisites

- **Node.js 22 LTS** or newer
- **npm 10+**
- **LLM Server**: Qwen2.5-3B-Instruct running via llama.cpp Docker (see [`docker-manage/`](./docker-manage/README.md))

## LLM Server Setup

The simulation requires a local LLM server for AI agent decisions. We use **Qwen2.5-3B-Instruct** (Q4_K_M quantization) running in Docker with CUDA:

```powershell
# From docker-manage directory
.\deploy.ps1
```

This downloads the model and starts llama.cpp on port 8080. See [`docker-manage/README.md`](./docker-manage/README.md) for full setup instructions.

**Performance**: ~180 tokens/sec on RTX 3090, ~300-400ms per agent decision.

## Installation

```bash
npm install
```

## Development Commands

| Command | Description |
|---------|-------------|
| `npm run dev:all` | Start both Fastify server (port 4000) and Vite client (port 5173) |
| `npm run dev:server` | Run only the server workspace (`@agentrunner/server`) |
| `npm run dev:client` | Run only the React client |
| `npm --workspace @agentrunner/server run probe` | Execute a headless simulation run to verify the engine |

## Build & Lint

| Command | Description |
|---------|-------------|
| `npm run build` | Type-check shared/server, then build the client bundle |
| `npm run lint` | Lint client, server, and shared TypeScript sources |

## Tests & Smoke Checks

| Command | Description |
|---------|-------------|
| `npm test` | Run the Vitest suite across shared/server packages |
| `npm run test:watch` | Watch mode for the test suite during development |
| `npm run smoke:test` | Launch full stack, poll `/health`, then shut down when ready |

---

## Manual QA Checklist

1. Run `npm run dev:all` to start the Fastify simulation server (port 4000) and Vite client (port 5173).
2. Open http://localhost:5173 in a Chromium-based browser and confirm:
   - Agents render on The Skeld map and continue moving for at least 30 seconds.
   - The Agent Activity panel updates without stalling (watch for frozen timestamps).
3. Open the browser devtools console and ensure the WebSocket handshake logs `connected` and periodic heartbeats from `SimulationClient`.
4. Simulate a dropped connection:
   - In devtools, toggle the Network tab to `Offline` for ~5 seconds, then return to `Online`.
   - Verify the console logs a `stale` state followed by a `connected` state after the connection recovers.
5. With the stack still running, open http://localhost:4000/analytics/metrics to view tick timing averages.
6. When finished, stop `npm run dev:all` with `Ctrl+C`.

---

## Architecture

### Workspace Layout

```
agentrunner/
â”œâ”€â”€ server/           # Fastify WebSocket server + simulation engine
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ ai/              # LLM integration, decision prompts
â”‚       â”œâ”€â”€ simulation/      # GameSimulation, SimulationLoop
â”‚       â””â”€â”€ observability/   # Telemetry, state history
â”œâ”€â”€ shared/           # TypeScript contracts shared between client/server
â”‚   â”œâ”€â”€ engine/              # AI agents, pathfinding, movement, state machines
â”‚   â”œâ”€â”€ types/               # Game types, protocol types, simulation types
â”‚   â””â”€â”€ data/                # Map data (The Skeld polygons, tasks, vents)
â”œâ”€â”€ src/              # React + PixiJS client (rendering only)
â”‚   â”œâ”€â”€ components/          # AgentInfoPanel, UI elements
â”‚   â””â”€â”€ rendering/           # PixiJS renderers for map, agents, vision, etc.
â””â”€â”€ maps/             # Map editor tools and raw map data
```

### Tech Stack

- **Server**: Fastify, WebSocket, TypeScript
- **Client**: React 19, PixiJS 8, Zustand
- **AI**: Qwen2.5-3B-Instruct via llama.cpp Docker (CUDA)
- **Build**: Vite, Vitest, ESLint, TypeScript 5.9

### WebSocket Protocol

| Message Type | Direction | Description |
|--------------|-----------|-------------|
| `handshake` | Server â†’ Client | Protocol version, server time |
| `snapshot` | Server â†’ Client | Full world state (on connect) |
| `state-update` | Server â†’ Client | Delta updates (movement, AI state) |
| `heartbeat` | Bidirectional | Keep-alive with tick count |

### AI Agent Decision Types

Agents can pursue these goals based on LLM reasoning:
- `GO_TO_TASK` â€“ Navigate to assigned task
- `WANDER` â€“ Random exploration
- `FOLLOW_AGENT` â€“ Tail another agent
- `AVOID_AGENT` â€“ Stay away from someone
- `BUDDY_UP` â€“ Team up for safety
- `CONFRONT` â€“ Question suspicious behavior
- `SPREAD_RUMOR` â€“ Share suspicions with others
- `DEFEND_SELF` â€“ Provide alibis when accused
- `SPEAK` â€“ General conversation
- `IDLE` â€“ Wait and observe

---

## REST Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /health` | Health check |
| `GET /analytics/metrics` | Tick timing, delta sizes, agent counts |
| `GET /analytics/state-history` | Rolling buffer of past states for debugging |

---

## Related Documentation

- [`agents.md`](./agents.md) â€” Complete Among Us game mechanics reference for AI agents
- [`docker-manage/README.md`](./docker-manage/README.md) â€” LLM server deployment and management
- [`docker-manage/agents.md`](./docker-manage/agents.md) â€” Docker infrastructure documentation
- [`upgrade.md`](./upgrade.md) â€” Migration checklist and progress
- [`maps/README.md`](./maps/README.md) â€” Map editor documentation

---

## License

Private project â€“ see repository for details.
